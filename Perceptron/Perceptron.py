import numpy as np
np.set_printoptions(suppress=True)
class Perceptron(object):
    """Perceptron classifier.

    Parameters
    ------------
    eta:float
        Learning rate (between 0.0 and 1.0)
    n_iter:int
        Passes over the training dataset.

    Attributes
    -------------
    w_: 1d-array
        Weights after fitting.
    errors_: list
        Numebr of misclassifications in every epoch.

    """

    def __init__(self, eta=0.9, n_iter=10):
        self.eta = eta
        self.n_iter = n_iter

#         self.w_ = np.array([-0.2,-0.00006419,-0.00005734,-0.00026331,-0.00003917,0.00021433,0.00239968
# ,0.00333655,0.00172586,0.00149352,0.00168192,0.00176625,0.00109687
# ,0.00024495,-0.00029253,-0.00167563,-0.001157,0.00112217,0.00116325
# ,0.00442885,0.00785925,0.0085283,0.00376401,0.00189145,-0.00032513
# ,-0.00075226,-0.0006692,-0.0001859,0.00016041,-0.00006397,-0.00111139
# ,-0.00259457,-0.00390699,-0.00117228,0.00576078,0.00511057,0.00267697
# ,0.0042232,0.00666718,0.00626086,0.00700139,0.00030537,0.00162301
# ,-0.00748766,-0.01151806,-0.01023678,-0.01190721,-0.01036022,-0.00312098
# ,0.00301228,0.00314011,-0.00116895,-0.00343207,-0.0060942,-0.00618579
# ,-0.00439477,-0.00027366,0.00005098,-0.00176125,-0.00686547,-0.00483989
# ,0.00655701,0.00003259,-0.00702599,-0.00314267,-0.00305466,-0.01244035
# ,-0.01383732,0.00233216,-0.0020852,0.00545862,0.00226863,0.01387275
# ,0.01318034,-0.00107847,-0.00085031,0.0090339,0.00435862,-0.00013099
# ,-0.00391838,0.00071869,-0.00141888,-0.00376016,-0.00705483,-0.00227809
# ,0.00125574,0.00359445,0.00742014,0.00642288,0.01077002,-0.008608
# ,-0.00318822,0.01022169,-0.0007076,-0.00051409,0.00495343,-0.00605554
# ,-0.01739163,-0.00832604,-0.00816266,0.01922169,0.02496632,0.0100853
# ,-0.00606071,0.00608058,-0.01480307,-0.01354153,-0.01261894,-0.01093509
# ,-0.00332817,-0.00361493,-0.00455414,-0.00596883,0.0087702,0.00587902
# ,-0.00047663,-0.01375975,-0.0107375,0.00425902,0.00274847,-0.00701489
# ,-0.00339595,0.01049796,0.0075982,-0.00644324,-0.00622091,-0.00118461
# ,-0.00192129,0.00581319,0.01465602,0.01155856,-0.01866475,-0.00039074
# ,-0.00310686,-0.00292067,-0.00624025,-0.00919097,-0.0102634,-0.0105927
# ,-0.003064,-0.00807455,0.00091286,0.00205308,-0.0035807,-0.01680645
# ,-0.00276349,0.0012231,-0.00487228,-0.00132684,0.00707358,-0.00179702
# ,-0.00811396,-0.00209821,0.00790527,0.01504063,0.02277931,0.00159777
# ,0.00432187,-0.00417558,-0.00668115,0.0041579,-0.01364994,-0.00839191
# ,-0.00343421,0.00559536,-0.01089444,-0.01173627,-0.00466145,-0.0081906
# ,0.00043172,0.00300144,-0.00293416,-0.01842598,0.00050515,-0.0066542
# ,-0.0094578,-0.01782811,-0.01097391,-0.01156965,-0.01453327,0.00584375
# ,0.01188314,-0.0040719,-0.00604935,-0.01589936,0.00630551,-0.01102266
# ,-0.02160693,0.00309269,-0.02242415,-0.01314823,-0.01091903,0.00335645
# ,-0.00008345,-0.01546778,-0.01063151,-0.00599523,0.00089979,0.00335655
# ,-0.00394682,-0.02380297,-0.01146408,-0.00886931,-0.00338232,0.00568324
# ,0.00813386,0.00727918,-0.02215783,0.00247515,-0.00723234,0.00666912
# ,0.01155899,0.00697728,0.00579393,0.00837032,-0.00399801,0.01791064
# ,0.00928121,0.01308883,0.00407661,-0.00313657,0.00404628,-0.00214083
# ,-0.00381676,-0.00221547,0.00053798,0.00545564,0.00083746,-0.00900041
# ,-0.014849,0.01087821,0.00356091,0.00060147,0.00631562,0.00293331
# ,0.00178187,0.00317145,-0.0096163,-0.01037619,0.01710628,0.01312327
# ,-0.02194014,-0.011558,-0.01998867,-0.00650283,-0.0146638,-0.00962555
# ,0.01160069,0.0143536,-0.00185824,0.00546666,0.00617012,0.00002615
# ,-0.00024992,0.00454596,0.00627608,-0.00963481,-0.02228639,-0.01144987
# ,-0.00810169,-0.00812784,0.00677463,-0.00838334,0.00258328,-0.00313495
# ,-0.00719216,-0.01019494,0.00891022,0.00823532,-0.00417361,0.00936312
# ,-0.00038853,-0.00837079,-0.00335861,-0.0183222,-0.00458589,-0.00096963
# ,0.0009193,0.00140477,0.00781761,0.00696155,0.00032675,0.00484191
# ,0.01198537,-0.00237198,-0.01651086,-0.01086413,-0.01211897,-0.00965357
# ,-0.00687758,-0.00514424,-0.00141929,-0.00003653,0.00890217,-0.01811018
# ,-0.01312579,0.00105746,-0.00639418,0.0087068,0.00810283,0.01775495
# ,0.00049968,0.00437363,0.01102281,-0.00730419,-0.00130383,0.00786364
# ,0.00545406,0.01055218,0.00320697,0.00832439,0.01111829,0.00802333
# ,0.00284085,0.01734324,0.0052017,0.01454359,0.00375723,-0.0065518
# ,-0.000276,-0.0018047,0.01011077,-0.0001334,-0.0080598,-0.00505744
# ,-0.00231741,0.00402034,-0.00025809,0.0292228,-0.00927362,0.01481556
# ,0.01076758,0.0033926,-0.00885104,0.01212856,0.00896435,0.00085436
# ,0.01105395,0.01029452,0.00519677,0.00677825,0.00136227,-0.0078213
# ,-0.01225669,0.00570812,-0.012704,-0.01486665,-0.00447773,0.00707785
# ,-0.0086861,-0.0145456,-0.01185784,-0.00163466,-0.00030742,0.00511473
# ,-0.00573011,0.01453133,0.00146189,0.01398127,-0.00495931,0.00422599
# ,-0.00365131,0.01299935,0.01196811,-0.00235868,0.00749452,0.01330788
# ,0.00617687,0.01404438,0.01223158,0.00099698,-0.00757513,0.00891095
# ,-0.00124051,0.00564226,0.01501932,0.00906321,-0.0050091,-0.00637231
# ,-0.01679267,-0.00477535,0.00075586,0.01330575,0.00017392,0.00052677
# ,0.01044384,0.00341012,-0.00177311,0.01143433,0.01098061,0.00841662
# ,0.01181746,-0.00263022,0.00472949,0.00824993,0.01144487,0.01929
# ,0.01197885,0.01164535,-0.0006768,0.00041155,-0.00267614,-0.00457548
# ,-0.00267785,-0.00682044,0.01706776,0.00236402,-0.01607632,0.00260045
# ,-0.01066252,-0.00479433,-0.00583564,0.00318012,0.00426268,0.01152926
# ,0.01709372,0.00641181,-0.00344273,0.00284247,0.00913191,-0.00185003
# ,0.00391668,0.00286031,0.01296039,0.00187098,0.00145833,0.00614659
# ,0.00998555,0.00340764,0.00393061,-0.00494899,-0.00518348,-0.00539477
# ,-0.00167525,-0.00732711,-0.02272052,-0.00405719,0.01264941,0.00248892
# ,-0.00888672,-0.02345568,0.00773515,-0.00425962,-0.00186014,-0.00440582
# ,-0.00768753,-0.00010011,0.00589402,-0.00053355,0.00303103,0.00396506
# ,0.01548519,-0.00537713,-0.00871489,-0.01610703,-0.00431543,-0.00078338
# ,-0.00344641,0.00436114,0.00155055,-0.0114508,0.00217166,0.00537377
# ,0.02221816,0.01153316,-0.00911212,-0.01493944,-0.01786512,0.01423798
# ,-0.00531732,-0.01175487,0.00895233,0.00208899,0.0040946,-0.00245315
# ,0.00235694,-0.00038474,0.00236747,0.00479868,0.01785114,-0.00059842
# ,-0.00091282,-0.0038059,0.00459819,0.00179208,-0.00577661,0.0025158
# ,0.00924036,-0.00499899,-0.02067247,0.01143724,0.01040454,0.00364877
# ,0.0069261,-0.00530075,-0.01521478,-0.00396225,-0.0303032,-0.00547663
# ,0.00131785,-0.00143119,-0.00025411,-0.00067009,0.00069347,0.00166568
# ,0.00098517,0.00194641,0.00870204,0.01035666,-0.01020761,-0.0127827
# ,0.00557023,0.0118895,-0.00322686,0.00948686,0.00382682,0.00617833
# ,-0.01678792,0.00190345,0.00915271,0.01464416,0.01006608,-0.00981022
# ,-0.00404258,0.00949876,0.01469008,0.00454007,-0.0024618,0.0055043
# ,-0.004122,-0.0015643,0.00020636,0.0107374,0.00027108,0.00167353
# ,-0.00621872,0.00654422,-0.00278512,-0.0107393,-0.0137019,0.00980301
# ,-0.0110355,-0.0012951,-0.01042354,-0.00305916,-0.00630832,0.00802588
# ,0.00881549,-0.00226354,-0.00316052,-0.01457625,0.00556157,0.01202794
# ,-0.00525509,-0.01762848,-0.01404387,-0.01045963,0.00170765,-0.00023624
# ,0.01235076,0.00548669,-0.00004489,0.00353998,0.00392211,-0.00143353
# ,-0.01699856,-0.00083087,-0.02620206,-0.00622157,0.00493358,0.01357231
# ,0.00084016,0.00469014,0.02350453,0.00945347,0.00187092,-0.00302412
# ,0.00249497,-0.00033288,0.01294205,0.01451179,0.01840923,0.00390626
# ,0.00560284,0.00902063,0.016764,-0.00159691,-0.00093408,-0.00170611
# ,-0.00035058,-0.00055848,0.0034772,0.00629903,-0.01633458,0.00613713
# ,0.00634553,-0.02159027,-0.00194734,0.01071574,0.00870158,-0.00572382
# ,-0.00577902,0.00147886,-0.00073212,0.00789717,0.01333777,0.00003716
# ,-0.00694184,-0.00024709,-0.01661004,-0.01550613,-0.01294114,-0.02011221
# ,-0.00157329,-0.00699197,-0.00118509,-0.00001131,-0.00009834,-0.00280581
# ,-0.00192232,0.00435238,-0.00616415,-0.00825431,0.0091395,-0.00192565
# ,-0.00896826,0.00204283,0.01262614,-0.00105947,0.00966925,0.00935115
# ,0.00346749,0.01331683,0.0149188,0.00268806,0.00281807,0.00950946
# ,0.00850644,0.01540573,0.00578946,-0.01119367,0.0039748,-0.00045143
# ,-0.00205011,-0.00008777,0.00013093,-0.00143353,-0.00537588,-0.00796383
# ,-0.00943519,-0.0088894,-0.00050076,-0.00192026,0.00554144,0.00890163
# ,0.00421482,0.01085953,0.01378848,0.00779701,0.00351479,0.01059245
# ,0.0061795,-0.0171463,-0.00814582,0.00227625,0.00049853,0.00524557
# ,-0.00647698,-0.00218851,-0.00284516,-0.00091411,-0.00272424,-0.00008777
# ,0.00016041,0.00015627,0.00003762,-0.00095556,-0.00321759,-0.00309849
# ,-0.00490764,-0.00151527,0.00168591,0.00762389,0.01213214,-0.00592416
# ,-0.0079677,-0.00278562,-0.00146861,-0.00026545,-0.00155401,0.0015337
# ,0.01128143,0.00905374,-0.00171762,-0.00782366,-0.00788837,-0.00344287
# ,-0.01352562,-0.00184433,-0.0027433,-0.00008777,0.00016041,0.00016041
# ,0.00016041,0.0000997,-0.00040492,-0.00001395,0.00394061,0.00280872
# ,0.00020352,0.00082305,0.0027671,0.00815075,0.00988896,0.00696397
# ,0.00107408,-0.00436171,-0.00821178,-0.01075498,-0.01473682,-0.0073476
# ,-0.00229107,-0.00283028,-0.00372757,-0.01430625,-0.00888155,-0.00229657
# ,-0.00269366,-0.00001332,0.00016041,0.00016041,0.00016041,0.00016041
# ,0.00006141,-0.00078845,-0.00058229,0.00203618,0.00407786,0.00472578
# ,0.00466565,0.00380627,0.0045532,0.00646532,0.01002452,0.01281286
# ,0.01174985,0.0103073,0.00168138,-0.0002164,-0.00503794,-0.00363305
# ,-0.0021745,-0.00936414,-0.00402946,-0.00381047,-0.00187467,0.00011077
# ,0.00016041,0.00016041,0.00016041,0.00016041,0.00010956,-0.00012921
# ,-0.00045822,-0.00114504,-0.00053349,0.00045225,0.00176549,0.00258216
# ,0.00285504,0.00369202,0.00403247,0.00379186,0.00498718,0.00670802
# ,0.00649342,0.00486125,0.00531294,0.00442747,-0.00102696,-0.00098604
# ,-0.00027639,-0.00038559,-0.00003813,0.00016041])

    def fit(self, X, y):
        """Fit training data.

        Parameters
        ------------
        X: {array-like}, shape=[n_samples, n_features]
            Training vectors, where n_samples is the number of samples
            and n_featuers is the number of features.
        y: array-like, shape=[n_smaples]
            Target values.

        Returns
        ----------
        self: object
        """

        self.w_ = np.zeros(1 + X.shape[1]) # Add w_0
        self.errors_ = []

        for _ in range(self.n_iter):
            errors = 0
            for xi, target in zip(X, y):
                update = self.eta * (target - self.predict(xi))

                self.w_[1:] += update * xi
                self.w_[0] += update
                errors += int(update != 0.0)
                # print(self.w_)
                # print("errors: ", errors)
            self.errors_.append(errors)
            print("self.errors: ",self.errors_)
        return self

    def net_input(self, X):
        """Calculate net input"""
        return np.dot(X, self.w_[1:]) + self.w_[0]

    def predict(self, X):
        """Return class label after unit step"""
        return np.where(self.net_input(X) >= 0.0, 1, -1)
